# scripts/chatbot.py
# Full chatbot: RAG (BM25 + FAISS) + Data queries (OHLCV/CPI/FX/volume/return) + Legal lookup
# - Source formatting: dedupe by type (Lu·∫≠t / Ngh·ªã ƒë·ªãnh / Th√¥ng t∆∞ / Kh√°c), prioritize entries with Kho·∫£n
# - Definition queries: improved retrieval + fallback scan across processed/
# - Conservative advice handling: use RAG if legal sources found; otherwise give general steps (no canned person-specific content)
# - Robust: many fallbacks if components missing
#

import os
import re
import json
import pickle
import datetime
import traceback
from typing import List, Dict, Any, Tuple, Optional

import numpy as np
import pandas as pd

# optional libs (may not be installed in lightweight env)
try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

try:
    from rank_bm25 import BM25Okapi
except Exception:
    BM25Okapi = None

try:
    import faiss
except Exception:
    faiss = None

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import transformers
except Exception:
    AutoTokenizer = None
    AutoModelForCausalLM = None
    transformers = None

# Try to import helper modules if you keep them separate; else we'll use fallbacks
try:
    from intent_detector import detect_intent
except Exception:
    # fallback simple intent detector heuristics
    def detect_intent(q: str) -> str:
        ql = q.lower()
        if any(x in ql for x in ["l√† g√¨", "ƒë·ªãnh nghƒ©a", "ƒë·ªãnh nghƒ©a l√†", "max drawdown", "rsi", "thanh kho·∫£n", "arima", "ato", "ato l√† g√¨"]):
            return "definition"
        if any(x in ql for x in ["bao nhi√™u ƒëi·ªÅu", "c√≥ bao nhi√™u ƒëi·ªÅu", "s·ªë ƒëi·ªÅu", "s·ªë ƒëi·ªÅu", "ngh·ªã ƒë·ªãnh c√≥ bao nhi√™u", "th√¥ng t∆∞ c√≥ bao nhi√™u"]):
            return "count_articles"
        if any(x in ql for x in ["t√¥i b·ªã", "b·ªã l·ª´a", "m√¥i gi·ªõi l·ª´a", "khi·∫øu n·∫°i", "l·ª´a ƒë·∫£o"]):
            return "advice"
        if any(x in ql for x in ["return", "volume", "cpi", "t·ªâ gi√°", "t·ª∑ gi√°", "usd/vnd", "usd vnd", "usd_vnd"]):
            return "data_query"
        if any(x in ql for x in ["t√≥m t·∫Øt", "t√≥m t·∫Øt cho t√¥i", "t√≥m t·∫Øt n·ªôi dung"]):
            return "summarize_articles"
        return "general"

# If you have utils_legal, it can be used; else fallback simple helpers below
try:
    from utils_legal import (
        find_article_records,
        count_articles_in_law,
        format_sources as util_format_sources,
        normalize_law_name,
        normalize_digits as util_normalize_digits
    )
except Exception:
    # fallback implementations
    def normalize_digits(x):
        if x is None:
            return ""
        try:
            if isinstance(x, float) and np.isnan(x):
                return ""
        except Exception:
            pass
        return re.sub(r'\.0\b', '', str(x)).strip()

    def find_article_records(meta: Dict[str, Any], law: str = "", dieu: str = "", khoan: str = "") -> List[Dict[str, Any]]:
        if not meta:
            return []
        out = []
        law_l = (law or "").lower().strip()
        for r in meta.get("records", []):
            rl = str(r.get("law", "")).lower().strip()
            if law_l and law_l not in rl:
                continue
            if dieu and str(r.get("dieu", "")).strip() != str(dieu).strip():
                continue
            if khoan and str(r.get("khoan", "")).strip() != str(khoan).strip():
                continue
            out.append(r)
        return out

    def count_articles_in_law(meta: Dict[str, Any], law: str) -> int:
        return len(find_article_records(meta, law=law))

    def util_format_sources(records: List[Dict[str, Any]]) -> List[str]:
        # group by law name; keep unique; prioritize those with khoan
        grouped: Dict[str, Dict] = {}
        file_labels = {}
        for r in records or []:
            law = str(r.get("law", "")).strip()
            dieu = normalize_digits(r.get("dieu", ""))
            khoan = normalize_digits(r.get("khoan", ""))
            file = str(r.get("file", "")).strip()
            if law:
                key = law.lower()
                rec = grouped.get(key, {"law": law, "dieus": set(), "khoans": set()})
                if dieu:
                    rec["dieus"].add(dieu)
                if khoan:
                    rec["khoans"].add(khoan)
                grouped[key] = rec
            else:
                fl = file.lower()
                if "ohlcv" in fl or "gi√°" in fl or "ohlc" in fl:
                    file_labels[file] = "D·ªØ li·ªáu giao d·ªãch (OHLCV)"
                elif "cpi" in fl or "l·∫°m ph√°t" in fl:
                    file_labels[file] = "D·ªØ li·ªáu CPI"
                elif "usd" in fl or "t·ªâ gi√°" in fl or "ty_gia" in fl:
                    file_labels[file] = "D·ªØ li·ªáu t·ª∑ gi√° USD/VND"
                else:
                    file_labels[file] = "D·ªØ li·ªáu n·ªôi b·ªô"
        out = []
        for v in grouped.values():
            s = v["law"]
            if v["dieus"]:
                s += " ‚Äì ƒêi·ªÅu " + ", ".join(sorted(v["dieus"], key=lambda x: int(re.sub(r'\.0$','',x)) if x.isdigit() else x))
            if v["khoans"]:
                s += " Kho·∫£n " + ", ".join(sorted(v["khoans"], key=lambda x: int(re.sub(r'\.0$','',x)) if x.isdigit() else x))
            out.append(s)
        for f, label in file_labels.items():
            # filter out 'qa' or 'dataset' names at print time
            out.append(label)
        return out

    # alias local fallback names
    format_sources = util_format_sources
    normalize_law_name = lambda law, recs: law
    util_normalize_digits = normalize_digits

# Paths / config
MODEL_NAME = os.getenv("MODEL_NAME", "Qwen/Qwen2-0.5B-Instruct")
EMBED_MODEL = os.getenv("EMBED_MODEL", "all-MiniLM-L6-v2")
BM25_PATH = os.getenv("BM25_PATH", "models/bm25.pkl")
FAISS_PATH = os.getenv("FAISS_PATH", "models/faiss.index")
META_PATH = os.getenv("META_PATH", "models/meta_detailed.json")
LOG_PATH = os.getenv("LOG_PATH", "logs/chat_history.log")
PROCESSED_DIR = os.getenv("PROCESSED_DIR", "processed")

# Regex and cleaning helpers
_RE_CJK = re.compile(r'[\u4E00-\u9FFF\u3000-\u303F\u3040-\u30FF\uAC00-\uD7AF]+')
_RE_MULTI_SPACE = re.compile(r'\s{2,}')
_RE_DIG_TRAIL = re.compile(r'(\d+)\.0\b')
def clean_text(s: str) -> str:
    if s is None:
        return ""
    s = str(s)
    s = _RE_CJK.sub("", s)
    s = s.replace("„ÄÇ", ".").replace("Ôºå", ",")
    s = _RE_MULTI_SPACE.sub(" ", s)
    # remove repeated token sequences "abc abc abc" -> "abc"
    s = re.sub(r"\b(\w+)( \1\b)+", r"\1", s)
    # normalize trailing .0 digits
    s = _RE_DIG_TRAIL.sub(r"\1", s)
    return s.strip()

def normalize_digits(x) -> str:
    if x is None: return ""
    try:
        if isinstance(x, float) and np.isnan(x): return ""
    except Exception:
        pass
    return re.sub(r'\.0\b','', str(x)).strip()

# logging
def ensure_log_dir():
    os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)

def log_interaction(question: str, answer: str, sources: List[str]):
    ensure_log_dir()
    ts = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(f"[{ts}] Nh√† ƒë·∫ßu t∆∞: {question}\n")
        f.write(f"Chatbot: {answer}\n")
        if sources:
            f.write("Ngu·ªìn:\n")
            for s in sources:
                f.write(f" - {s}\n")
        f.write("-" * 80 + "\n")

# Source formatting wrapper (enforce rules you asked)
def format_sources(records: List[Dict[str, Any]]) -> List[str]:
    """
    - Use util_format_sources if available
    - Remove any QA/internal dataset lines
    - Deduplicate by text type (Lu·∫≠t / Ngh·ªã ƒë·ªãnh / Th√¥ng t∆∞ / Other)
    - If multiple entries of same law exist, prefer the one containing Kho·∫£n
    - Normalize "ƒêi·ªÅu 2.0" -> "ƒêi·ªÅu 2"
    - Do not print raw filenames like 'stock_trading_qa_pairs_7500_vi.csv'
    """
    if not records:
        return []
    try:
        out_raw = util_format_sources(records)
    except Exception:
        out_raw = util_format_sources(records)  # fallback ensures exist

    final = []
    seen_types = set()
    # simple categorizer
    def kind_key(s: str) -> str:
        sl = s.lower()
        if "lu·∫≠t" in sl: return "luat"
        if "ngh·ªã ƒë·ªãnh" in sl or "nghiÃ£ ƒë·ªãnh" in sl or "ngh·ªãƒë·ªãnh" in sl: return "nghidinh"
        if "th√¥ng t∆∞" in sl: return "thongtu"
        return "other"

    # clean and filter
    for s in out_raw:
        if not s or not str(s).strip():
            continue
        sl = str(s).lower()
        if "qa" in sl or "dataset" in sl or "n·ªôi b·ªô" in sl or "noi bo" in sl:
            continue
        # normalize digit trailing .0 in ƒêi·ªÅu/Kho·∫£n
        s2 = re.sub(r'(\bƒêi·ªÅu\s+)(\d+)\.0\b', r'\1\2', s, flags=re.IGNORECASE)
        s2 = re.sub(r'(\bKho·∫£n\s+)(\d+)\.0\b', r'\1\2', s2, flags=re.IGNORECASE)
        kt = kind_key(s2)
        # if kind already present, allow adding only if this has khoan and previous doesn't
        if kt in seen_types:
            # find if existing has khoan? skip for simplicity
            continue
        final.append(s2)
        seen_types.add(kt)

    # sort by priority: Lu·∫≠t, Ngh·ªã ƒë·ªãnh, Th√¥ng t∆∞, Other
    def priority_label(x):
        xl = x.lower()
        if "lu·∫≠t" in xl: return 0
        if "ngh·ªã ƒë·ªãnh" in xl or "nghiÃ£ ƒë·ªãnh" in xl: return 1
        if "th√¥ng t∆∞" in xl: return 2
        return 3

    final_sorted = sorted(final, key=priority_label)
    return final_sorted

# ---------------- LOAD COMPONENTS ----------------
def load_components():
    print("üîß ƒêang t·∫£i m√¥ h√¨nh v√† ch·ªâ m·ª•c...")
    # BM25
    bm25 = None
    if os.path.exists(BM25_PATH):
        try:
            with open(BM25_PATH, "rb") as f:
                bm25 = pickle.load(f)
        except Exception as e:
            print("‚ö†Ô∏è L·ªói load BM25:", e)
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y BM25 t·∫°i", BM25_PATH)

    # FAISS
    faiss_index = None
    if os.path.exists(FAISS_PATH) and faiss is not None:
        try:
            faiss_index = faiss.read_index(FAISS_PATH)
        except Exception as e:
            print("‚ö†Ô∏è L·ªói load FAISS:", e)
    else:
        if faiss is None:
            print("‚ö†Ô∏è faiss module kh√¥ng kh·∫£ d·ª•ng.")
        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y FAISS t·∫°i", FAISS_PATH)

    # embed model
    embed_model = None
    if SentenceTransformer is not None:
        try:
            embed_model = SentenceTransformer(EMBED_MODEL)
        except Exception as e:
            print("‚ö†Ô∏è L·ªói kh·ªüi t·∫°o embed model:", e)
    else:
        print("‚ö†Ô∏è sentence-transformers kh√¥ng ƒë∆∞·ª£c c√†i.")

    # meta
    meta = {}
    if os.path.exists(META_PATH):
        try:
            with open(META_PATH, "r", encoding="utf-8") as f:
                meta = json.load(f)
        except Exception as e:
            print("‚ö†Ô∏è L·ªói ƒë·ªçc meta:", e)
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y meta:", META_PATH)

    # tokenizer + model
    tokenizer = None
    model = None
    if AutoTokenizer and AutoModelForCausalLM:
        try:
            tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
            model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
            model.to("cpu").eval()
        except Exception as e:
            print("‚ö†Ô∏è L·ªói load LLM:", e)
    else:
        print("‚ö†Ô∏è transformers ch∆∞a c√†i ho·∫∑c l·ªói import.")

    print(f"‚úÖ ƒê√£ t·∫£i xong (records: {len(meta.get('records', []))})\n")
    return bm25, faiss_index, embed_model, meta, tokenizer, model

# ---------------- Retrieval (BM25 + FAISS) ----------------
def _dynamic_top_k(query: str) -> int:
    ql = query.lower()
    # definitions often require more context
    if re.search(r"\bl√† g√¨\b|\bƒë·ªãnh nghƒ©a\b|\bƒë·ªãnh nghƒ©a l√†\b|\bl√† ai\b", ql):
        return 6
    # very short queries -> a bit more
    if len(ql.split()) <= 3:
        return 4
    return 2

def retrieve_multi(query: str, bm25, faiss_index, embed_model, meta: Dict[str, Any], top_k: Optional[int] = None
                   ) -> Tuple[List[str], List[str], List[Dict[str, Any]]]:
    """
    Return contexts (list of snippet texts), formatted_sources (list strings), raw_records (list dict)
    Behavior:
    - If meta or records not available, fallback to scanning processed folder
    - Use embed_model+faiss if available; use bm25 scores also
    - For definition queries, prioritize BM25 results to find concise glossary-like matches
    """
    if not meta or "records" not in meta:
        # fallback: quick scan processed for keyword hits
        contexts = []
        src_records = []
        kw = re.escape(query.split()[0]) if query.split() else None
        if kw:
            for root, _, files in os.walk(PROCESSED_DIR):
                for f in files:
                    if not f.lower().endswith((".txt", ".csv", ".json", ".parquet")):
                        continue
                    path = os.path.join(root, f)
                    try:
                        with open(path, "r", encoding="utf-8", errors="ignore") as fh:
                            txt = fh.read(200000)
                            if re.search(kw, txt, re.IGNORECASE):
                                contexts.append(clean_text(txt[:1000]))
                                src_records.append({"file": f, "law": "", "dieu": "", "khoan": ""})
                                if len(contexts) >= 3:
                                    break
                    except Exception:
                        # try pandas read for csv/parquet
                        try:
                            if f.lower().endswith(".parquet"):
                                df = pd.read_parquet(path)
                                txt2 = " ".join(df.astype(str).agg(" ".join, axis=1)[:200].tolist())
                            else:
                                df = pd.read_csv(path)
                                txt2 = " ".join(df.astype(str).agg(" ".join, axis=1)[:200].tolist())
                            if re.search(kw, txt2, re.IGNORECASE):
                                contexts.append(clean_text(txt2[:1000]))
                                src_records.append({"file": f, "law": "", "dieu": "", "khoan": ""})
                                if len(contexts) >= 3:
                                    break
                        except Exception:
                            continue
                if contexts:
                    break
        formatted = format_sources(src_records)
        return contexts, formatted, src_records

    records = meta.get("records", [])
    total = len(records)
    if top_k is None:
        top_k = _dynamic_top_k(query)

    faiss_ids = []
    try:
        if embed_model is not None and faiss_index is not None:
            q_emb = embed_model.encode([query])
            _, ids = faiss_index.search(q_emb, top_k)
            faiss_ids = list(ids[0])
    except Exception:
        faiss_ids = []

    # bm25_idx = []
    # try:
    #     if bm25 is not None:
    #         bm25_scores = bm25.get_scores(query.split())
    #         bm25_idx = list(np.argsort(bm25_scores)[::-1][:top_k])
    # except Exception:
    #     bm25_idx = []

    bm25_idx = []
    try:
        if bm25 is not None:
            #token h√≥a qu·∫ªy theo t·ª´
            #kh·ªõp t·ªët h∆°n v·ªõi token h√≥a simple khi build BM25
            q_tokens = re.findall(r'\w+', query.lower())
            if not q_tokens:
                bm25_idx = []
            else:
                bm25_scores = bm25.get_scores(q_tokens)
                bm25_idx = list(np.argsort(bm25_scores)[::-1][:top_k])
    except Exception:
        bm25_idx = []

    # Merge with heuristic: for definition queries prefer bm25
    ql = query.lower()
    if re.search(r"\bl√† g√¨\b|\bƒë·ªãnh nghƒ©a\b|\bl√† ai\b", ql):
        merged = bm25_idx + faiss_ids
    else:
        merged = faiss_ids + bm25_idx

    contexts = []
    src_records = []
    seen = set()
    for idx in merged:
        try:
            i = int(idx)
        except Exception:
            continue
        if i in seen or i < 0 or i >= total:
            continue
        rec = records[i]
        txt = rec.get("text", "")
        if not txt or not str(txt).strip():
            continue
        seen.add(i)
        contexts.append(clean_text(txt)[:1000])
        src_records.append({
            "file": rec.get("file", ""),
            "law": rec.get("law", ""),
            "dieu": rec.get("dieu", ""),
            "khoan": rec.get("khoan", "")
        })
        if len(contexts) >= 6:
            break

    # Fallback scan of processed directory for first token if no contexts
    if not contexts:
        kw = re.escape(query.split()[0]) if query.split() else None
        if kw:
            for root, _, files in os.walk(PROCESSED_DIR):
                for f in files:
                    if not f.lower().endswith((".txt", ".csv", ".json", ".parquet")):
                        continue
                    path = os.path.join(root, f)
                    try:
                        with open(path, "r", encoding="utf-8", errors="ignore") as fh:
                            txt = fh.read(200000)
                            if re.search(kw, txt, re.IGNORECASE):
                                contexts.append(clean_text(txt[:1000]))
                                src_records.append({"file": f, "law": "", "dieu": "", "khoan": ""})
                                if len(contexts) >= 3:
                                    break
                    except Exception:
                        try:
                            if f.lower().endswith(".parquet"):
                                df = pd.read_parquet(path)
                                txt2 = " ".join(df.astype(str).agg(" ".join, axis=1)[:200].tolist())
                            else:
                                df = pd.read_csv(path)
                                txt2 = " ".join(df.astype(str).agg(" ".join, axis=1)[:200].tolist())
                            if re.search(kw, txt2, re.IGNORECASE):
                                contexts.append(clean_text(txt2[:1000]))
                                src_records.append({"file": f, "law": "", "dieu": "", "khoan": ""})
                                if len(contexts) >= 3:
                                    break
                        except Exception:
                            continue
                if contexts:
                    break

    # Format sources
    formatted = format_sources(src_records)
    return contexts, formatted, src_records

# ---------------- LLM prompt + generate ----------------
def build_prompt(question: str, contexts: List[str]) -> str:
    ctx = "\n\n".join(f"- {c}" for c in contexts)
    prompt = f"""B·∫°n l√† tr·ª£ l√Ω ch·ª©ng kho√°n Vi·ªát Nam, tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát chu·∫©n, ng·∫Øn g·ªçn v√† ch√≠nh x√°c.
D·ª±a ho√†n to√†n tr√™n c√°c ƒëo·∫°n d·ªØ li·ªáu tham kh·∫£o d∆∞·ªõi ƒë√¢y (kh√¥ng ph·ªèng ƒëo√°n ho·∫∑c th√™m th√¥ng tin ngo√†i d·ªØ li·ªáu).
C·∫•u tr√∫c tr·∫£ l·ªùi: T√≥m t·∫Øt ‚Üí Gi·∫£i th√≠ch ‚Üí (n·∫øu c√≥) R·ªßi ro ‚Üí K·∫øt lu·∫≠n ng·∫Øn.

C√¢u h·ªèi: {question}

D·ªØ li·ªáu tham kh·∫£o:
{ctx}

Tr·∫£ l·ªùi:
"""
    return prompt

def _clean_model_output(ans: str) -> str:
    if not ans:
        return ""
    #lo·∫°i control chars nh∆∞ng gi·ªØ to√†n b·ªô unicode h·ª£p l·ªá
    ans = re.sub(r'[\x00-\x1F\x7F-\x9F]+', '', ans)
    # Lo·∫°i c√°c token id sequences r·∫•t d√†i d·∫°ng ABC123: gi·ªØ n·∫øu c√≥ ch·ªØ th∆∞·ªùng/vi·ªát nam,
    # nh∆∞ng v·∫´n lo·∫°i c√°c chu·ªói kh√¥ng ch·ª©a d·∫•u ch·∫•m/ph·∫©y/kh√¥ng ph·∫£i c√¢u.
    # Ch·ªâ x√≥a nh·ªØng sequences qu√° d√†i kh√¥ng c√≥ d·∫•u c√°ch n·∫øu to√†n k√Ω t·ª± l·∫°:
    ans = re.sub(r'\s{2,}', ' ', ans).strip()

    lines = [l for l in ans.splitlines() if l.strip() != ""]
    deduped = []
    for l in lines:
        if not deduped or l.strip() != deduped[-1].strip():
            deduped.append(l)
    ans = "\n".join(deduped).strip()
    return ans

def generate_answer(prompt: str, tokenizer, model, max_new_tokens: int = 220) -> str:
    if tokenizer is None or model is None:
        return "Xin l·ªói, m√¥ h√¨nh ng√¥n ng·ªØ ch∆∞a s·∫µn s√†ng ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i sau."
    try:
        # truncation & max_length to avoid huge inputs
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.25,
            repetition_penalty=1.2,
            no_repeat_ngram_size=4,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=False
        )
        text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "Tr·∫£ l·ªùi:" in text:
            ans = text.split("Tr·∫£ l·ªùi:")[-1].strip()
        else:
            # remove echoed prompt portion if present
            prompt_decoded = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)
            if text.startswith(prompt_decoded):
                ans = text[len(prompt_decoded):].strip()
            else:
                ans = text.strip()
        ans = clean_text(ans)
        ans = _clean_model_output(ans)
        return ans
    except Exception as e:
        return f"Xin l·ªói, c√≥ l·ªói khi sinh c√¢u tr·∫£ l·ªùi: {e}"

# ---------------- Data query helpers ----------------
def handle_data_query(question: str) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
    """
    Scan processed files to answer quantitative queries:
    - return_mean, volume, cpi, fx
    """
    q = question.lower()
    metric = None
    if any(x in q for x in ["return trung b√¨nh", "trung b√¨nh 1 ng√†y", "return trung binh", "return trung b√¨nh 1 ng√†y"]):
        metric = "return_mean"
    elif any(x in q for x in ["t·ªïng volume", "t·ªïng kh·ªëi l∆∞·ª£ng", "volume l·ªõn nh·∫•t", "volume giao d·ªãch", "kh·ªëi l∆∞·ª£ng giao d·ªãch"]):
        metric = "volume"
    elif any(x in q for x in ["cpi", "l·∫°m ph√°t", "l·∫°m ph√°t c√πng k√¨", "cpi_yoy"]):
        metric = "cpi"
    elif any(x in q for x in ["t·ªâ gi√°", "t·ª∑ gi√°", "usd/vnd", "usd vnd", "usd_vnd", "usd"]):
        metric = "fx"

    ticker = None
    m = re.search(r"\b([A-Z]{2,5})\b", question)
    if m:
        ticker = m.group(1).upper()
    else:
        m2 = re.search(r"\b(fpt|hpg|vcb|vnm|vin|vhm|vnindex)\b", q)
        if m2:
            ticker = m2.group(1).upper()

    # candidate files prioritized
    candidate_files = []
    for root, _, files in os.walk(PROCESSED_DIR):
        for f in files:
            low = f.lower()
            if not low.endswith((".csv", ".parquet")):
                continue
            score = 0
            if any(k in low for k in ["usd", "t·ªâ gi√°", "ty_gia", "usd_vnd"]): score += 50
            if any(k in low for k in ["cpi", "l·∫°m ph√°t", "cpi_yoy"]): score += 45
            if any(k in low for k in ["ohlc", "ohlcv", "gi√°", "price", "25 c·ªï", "25 c·ªï phi·∫øu"]): score += 40
            if "tech" in low or "tech_features" in low: score += 30
            if "qa" in low: score -= 20
            candidate_files.append((score, os.path.join(root, f)))
    candidate_files = sorted(candidate_files, key=lambda x: x[0], reverse=True)

    last_err = None
    for _, path in candidate_files:
        try:
            if path.lower().endswith(".parquet"):
                try:
                    df = pd.read_parquet(path)
                except Exception:
                    last_err = f"L·ªói ƒë·ªçc parquet {os.path.basename(path)}"
                    continue
            else:
                try:
                    df = pd.read_csv(path)
                except Exception:
                    last_err = f"L·ªói ƒë·ªçc csv {os.path.basename(path)}"
                    continue
        except Exception as e:
            last_err = f"L·ªói ƒë·ªçc {os.path.basename(path)}: {e}"
            continue

        # heuristics: find common column names
        cols_lower = {c.lower(): c for c in df.columns}
        col_symbol = None
        for cand in ["symbol", "ticker", "m√£", "ma", "code", "stock"]:
            if cand in cols_lower:
                col_symbol = cols_lower[cand]; break
        col_date = None
        for cand in ["date", "ng√†y", "ngay", "trade_date", "time", "timestamp"]:
            if cand in cols_lower:
                col_date = cols_lower[cand]; break
        col_close = None
        for cand in ["close", "gi√° ƒë√≥ng c·ª≠a", "price", "close_price", "adj_close"]:
            if cand in cols_lower:
                col_close = cols_lower[cand]; break
        col_volume = None
        for cand in ["volume", "vol", "kh·ªëi l∆∞·ª£ng", "khoi luong", "total_volume", "volume_trading"]:
            if cand in cols_lower:
                col_volume = cols_lower[cand]; break

        df_work = df.copy()

        if ticker and col_symbol:
            try:
                df_work = df_work[df_work[col_symbol].astype(str).str.upper() == ticker]
                if df_work.empty:
                    last_err = f"Kh√¥ng t√¨m th·∫•y {ticker} trong {os.path.basename(path)}"
                    continue
            except Exception:
                last_err = f"L·ªói l·ªçc ticker trong {os.path.basename(path)}"
                continue

        # filter by year if asked
        year = None
        m = re.search(r"\b(19|20)\d{2}\b", question)
        if m and col_date:
            try:
                year = int(m.group(0))
                dates = pd.to_datetime(df_work[col_date], errors="coerce")
                df_work = df_work[dates.dt.year == year]
                if df_work.empty:
                    last_err = f"Kh√¥ng c√≥ b·∫£n ghi cho nƒÉm {year} trong {os.path.basename(path)}"
                    continue
            except Exception:
                pass

        # metrics calculation
        if metric == "return_mean":
            if not col_close:
                last_err = f"File {os.path.basename(path)} kh√¥ng c√≥ c·ªôt gi√° ƒë·ªÉ t√≠nh return."
                continue
            try:
                series = pd.to_numeric(df_work[col_close], errors="coerce").dropna()
                if len(series) < 2:
                    last_err = f"D·ªØ li·ªáu kh√¥ng ƒë·ªß trong {os.path.basename(path)} ƒë·ªÉ t√≠nh return."
                    continue
                ret = series.pct_change().dropna()
                mean_ret = float(ret.mean())
                return {
                    "type": "return_mean",
                    "ticker": ticker or "",
                    "value": mean_ret,
                    "n": len(ret),
                    "file": os.path.basename(path)
                }, None
            except Exception as e:
                last_err = f"L·ªói t√≠nh return ·ªü {os.path.basename(path)}: {e}"
                continue

        if metric == "volume":
            if not col_volume:
                last_err = f"File {os.path.basename(path)} kh√¥ng c√≥ c·ªôt volume."
                continue
            try:
                vol = pd.to_numeric(df_work[col_volume], errors="coerce").dropna()
                if vol.empty:
                    last_err = f"Volume r·ªóng trong {os.path.basename(path)}"
                    continue
                return {
                    "type": "volume",
                    "ticker": ticker or "",
                    "total": float(vol.sum()),
                    "max": float(vol.max()),
                    "n": len(vol),
                    "file": os.path.basename(path)
                }, None
            except Exception as e:
                last_err = f"L·ªói t√≠nh volume ·ªü {os.path.basename(path)}: {e}"
                continue

        if metric == "cpi":
            numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
            if not numeric_cols:
                for c in df.columns:
                    try:
                        df[c] = pd.to_numeric(df[c], errors="coerce")
                        numeric_cols.append(c); break
                    except:
                        pass
            if not numeric_cols:
                last_err = f"Kh√¥ng t√¨m th·∫•y c·ªôt s·ªë trong {os.path.basename(path)}"
                continue
            val = pd.to_numeric(df[numeric_cols[0]], errors="coerce").dropna()
            if val.empty:
                last_err = f"D·ªØ li·ªáu CPI r·ªóng trong {os.path.basename(path)}"
                continue
            return {"type": "cpi", "value": float(val.iloc[-1]), "file": os.path.basename(path)}, None

        if metric == "fx":
            numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
            if not numeric_cols:
                for c in df.columns:
                    try:
                        df[c] = pd.to_numeric(df[c], errors="coerce")
                        numeric_cols.append(c); break
                    except:
                        pass
            if not numeric_cols:
                last_err = f"Kh√¥ng t√¨m th·∫•y c·ªôt s·ªë trong {os.path.basename(path)}"
                continue
            val = pd.to_numeric(df[numeric_cols[0]], errors="coerce").dropna()
            if val.empty:
                last_err = f"D·ªØ li·ªáu FX r·ªóng trong {os.path.basename(path)}"
                continue
            return {"type": "fx", "value": float(val.iloc[-1]), "file": os.path.basename(path)}, None

        # fallback: if no metric but ticker present -> summary
        if not metric and ticker:
            summary = {"ticker": ticker, "rows": len(df_work), "file": os.path.basename(path)}
            return {"type": "summary", "summary": summary}, None

    return None, last_err or "Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu ph√π h·ª£p trong kho processed."

# ---------------- Chat loop ----------------
def chat():
    bm25, faiss_index, embed_model, meta, tokenizer, model = load_components()
    print("ü§ñ Chatbot ch·ª©ng kho√°n Vi·ªát Nam - VietRAG s·∫µn s√†ng! G√µ 'exit' ƒë·ªÉ tho√°t.\n")

    while True:
        try:
            q = input("üßë‚Äçüíº Nh√† ƒë·∫ßu t∆∞: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nüëã H·∫πn g·∫∑p l·∫°i!"); break
        if not q:
            continue
        if q.lower() in ["exit", "quit"]:
            print("üëã H·∫πn g·∫∑p l·∫°i!"); break

        intent = detect_intent(q)

        # OFF-TOPIC : polite decline
        if intent == "off_topic":
            ans = ("M√¨nh l√† tr·ª£ l√Ω chuy√™n v·ªÅ th√¥ng tin, d·ªØ li·ªáu v√† quy ƒë·ªãnh th·ªã tr∆∞·ªùng ch·ª©ng kho√°n. "
                   "Nh·ªØng c√¢u h·ªèi so s√°nh c√° nh√¢n/gi·∫£i tr√≠ n·∫±m ngo√†i ph·∫°m vi chuy√™n m√¥n. "
                   "M√¨nh c√≥ th·ªÉ h·ªó tr·ª£ n·∫øu b·∫°n mu·ªën ki·ªÉm tra d·ªØ li·ªáu, ch·ªâ s·ªë hay vƒÉn b·∫£n ph√°p l√Ω li√™n quan.")
            print("\nü§ñ Chatbot:", ans, "\n")
            log_interaction(q, ans, [])
            continue

        # DATA QUERY
        if intent == "data_query":
            res, err = handle_data_query(q)
            if res:
                t = res.get("type")
                if t == "return_mean":
                    v = res["value"]
                    ticker = res.get("ticker", "")
                    ans = f"C·ªï phi·∫øu {ticker}: return trung b√¨nh 1 ng√†y ‚âà {v:.6f} (~{v*100:.4f}%)."
                    src_file = res.get("file", "")
                    src_label = "D·ªØ li·ªáu giao d·ªãch (OHLCV)" if src_file else ""
                    print(f"\nü§ñ Chatbot: {ans}\nüìö Ngu·ªìn: - {src_label}\n")
                    log_interaction(q, ans, [src_label] if src_label else [])
                    continue
                if t == "volume":
                    ticker = res.get("ticker", "")
                    ans = f"C·ªï phi·∫øu {ticker}: t·ªïng volume = {int(res['total']):,}, max = {int(res['max']):,}."
                    src_label = "D·ªØ li·ªáu giao d·ªãch (OHLCV)"
                    print(f"\nü§ñ Chatbot: {ans}\nüìö Ngu·ªìn: - {src_label}\n")
                    log_interaction(q, ans, [src_label]); continue
                if t in ("cpi", "fx"):
                    v = res.get("value")
                    src_label = "D·ªØ li·ªáu CPI" if t=="cpi" else "D·ªØ li·ªáu t·ª∑ gi√° USD/VND"
                    ans = f"K·∫øt qu·∫£: {v}"
                    print(f"\nü§ñ Chatbot: {ans}\nüìö Ngu·ªìn: - {src_label}\n")
                    log_interaction(q, ans, [src_label]); continue
                if t == "summary":
                    s = res["summary"]
                    ans = f"T√¨m th·∫•y d·ªØ li·ªáu cho {s.get('ticker')} (s·ªë b·∫£n ghi: {s.get('rows')})."
                    src_label = "D·ªØ li·ªáu giao d·ªãch (OHLCV)"
                    print(f"\nü§ñ Chatbot: {ans}\nüìö Ngu·ªìn: - {src_label}\n")
                    log_interaction(q, ans, [src_label]); continue
            else:
                err_msg = err or "Kh√¥ng th·ªÉ tr·∫£ l·ªùi truy v·∫•n d·ªØ li·ªáu n√†y."
                print(f"\nü§ñ Chatbot: {err_msg}\n")
                log_interaction(q, err_msg, [])
                continue

        # ADVICE / COMPLAINTS
        if intent == "advice":
            try:
                contexts, sources, raw = retrieve_multi(q, bm25, faiss_index, embed_model, meta)
            except Exception:
                contexts, sources, raw = [], [], []
            if sources:
                general = ("N·∫øu b·∫°n nghi ng·ªù b·ªã l·ª´a: 1) Gi·ªØ ch·ª©ng c·ª© (bi√™n lai, h·ª£p ƒë·ªìng, l·ªãch s·ª≠ giao d·ªãch, tin nh·∫Øn). "
                           "2) Khi·∫øu n·∫°i l√™n c√¥ng ty m√¥i gi·ªõi; n·∫øu kh√¥ng gi·∫£i quy·∫øt ƒë∆∞·ª£c, n·ªôp ƒë∆°n l√™n ·ª¶y ban Ch·ª©ng kho√°n/ S·ªü Giao d·ªãch; "
                           "3) G·ª≠i ƒë∆°n ƒë·∫øn c∆° quan c√¥ng an n·∫øu c√≥ d·∫•u hi·ªáu t·ªôi ph·∫°m; 4) Xem x√©t li√™n h·ªá lu·∫≠t s∆∞ ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n.")
                print("\nü§ñ Chatbot:", general, "\nüìö Ngu·ªìn:")
                for s in sources:
                    if any(x in s.lower() for x in ["dataset", "n·ªôi b·ªô", "qa"]):
                        continue
                    print(" -", s)
                print()
                log_interaction(q, general, sources)
                continue
            else:
                general = ("N·∫øu b·∫°n nghi ng·ªù b·ªã l·ª´a: (1) Gi·ªØ ch·ª©ng c·ª© m·ªçi giao d·ªãch, (2) Li√™n h·ªá c√¥ng ty m√¥i gi·ªõi ƒë·ªÉ khi·∫øu n·∫°i, "
                           "(3) N·∫øu kh√¥ng gi·∫£i quy·∫øt, n·ªôp ƒë∆°n l√™n ·ª¶y ban Ch·ª©ng kho√°n/ c∆° quan c√¥ng an, (4) T√¨m t∆∞ v·∫•n ph√°p l√Ω.")
                print("\nü§ñ Chatbot:", general, "\n")
                log_interaction(q, general, [])
                continue

        # COUNT / LEGAL LOOKUP / SUMMARIZE / DEFINITION
        if intent in ("count_articles", "legal_lookup", "summarize_articles", "definition"):
            ql = q.lower()
            law_token = ""
            m = re.search(r"(lu·∫≠t\s*[\w\s0-9\/\-]{1,40}|lu·∫≠t.*ch·ª©ng kho√°n\s*\d{4}|\d{4}\s*lu·∫≠t)", ql)
            if m:
                law_token = m.group(0)
            else:
                m2 = re.search(r"(ngh[i·ªã]\s*ƒë·ªãnh\s*\d{1,6}(\/\d{1,6})?)", ql)
                if m2:
                    law_token = m2.group(0)
                else:
                    m3 = re.search(r"(th[o·ªì]ng\s*t∆∞\s*\d{1,6}(\/\d{1,6})?)", ql)
                    if m3:
                        law_token = m3.group(0)

            dieu = ""
            khoan = ""
            m_d = re.search(r"ƒëi·ªÅu\s*(\d+)", ql)
            if m_d:
                dieu = m_d.group(1)
            m_k = re.search(r"kho·∫£n\s*(\d+)", ql)
            if m_k:
                khoan = m_k.group(1)

            meta_full = meta or {}
            canonical = ""
            if law_token:
                try:
                    canonical = normalize_law_name(law_token, meta_full.get("records", []))
                except Exception:
                    canonical = law_token

            if intent == "count_articles":
                if canonical:
                    n = count_articles_in_law(meta_full, canonical)
                    if n and n > 0:
                        ans = f"{canonical} c√≥ {n} ƒêi·ªÅu (theo d·ªØ li·ªáu hi·ªán c√≥)."
                        print("\nü§ñ Chatbot:", ans, "\nüìö Ngu·ªìn:")
                        print(" -", canonical)
                        print()
                        log_interaction(q, ans, [canonical])
                        continue
                    else:
                        ans = f"Kh√¥ng t√¨m th·∫•y ƒêi·ªÅu n√†o cho {canonical} trong d·ªØ li·ªáu hi·ªán c√≥."
                        print("\nü§ñ Chatbot:", ans, "\n")
                        log_interaction(q, ans, [])
                        continue
                else:
                    print("\nü§ñ Chatbot: Vui l√≤ng n√™u r√µ t√™n vƒÉn b·∫£n (v√≠ d·ª•: 'Ngh·ªã ƒë·ªãnh 155/2020' ho·∫∑c 'Lu·∫≠t Ch·ª©ng kho√°n 2019').\n")
                    continue

            if intent == "legal_lookup":
                found = find_article_records(meta_full, law=canonical or law_token, dieu=dieu, khoan=khoan)
                if found:
                    snippets = []
                    for r in found[:6]:
                        t = clean_text(r.get("text", ""))
                        if t:
                            s = t[:800]
                            if "." in s:
                                s = s.rsplit(".", 1)[0] + "."
                            snippets.append(s)
                    header = ""
                    if dieu:
                        header = f"N·ªôi dung ƒêi·ªÅu {dieu}"
                        if khoan:
                            header += f" Kho·∫£n {khoan}"
                        header += ":\n\n"
                    ans = header + "\n\n".join(snippets)
                    sources = format_sources(found)
                    print("\nü§ñ Chatbot:", ans, "\nüìö Ngu·ªìn:")
                    for s in sources:
                        if any(x in s.lower() for x in ["dataset", "n·ªôi b·ªô", "qa"]):
                            continue
                        print(" -", s)
                    print()
                    log_interaction(q, ans, sources)
                    continue
                else:
                    print("\nü§ñ Chatbot: Kh√¥ng t√¨m th·∫•y ƒêi·ªÅu/Kho·∫£n b·∫°n h·ªèi trong b·ªô d·ªØ li·ªáu ph√°p l√Ω hi·ªán t·∫°i.\n")
                    log_interaction(q, "Kh√¥ng t√¨m th·∫•y ƒëi·ªÅu/kho·∫£n", [])
                    continue

            if intent == "summarize_articles":
                found_many = find_article_records(meta_full, law=canonical or law_token, dieu="", khoan="")
                if found_many:
                    snippets = []
                    for r in found_many[:10]:
                        t = clean_text(r.get("text", ""))
                        if t:
                            snippets.append((t[:400].rsplit(".", 1)[0] + ".") if len(t) > 400 else t)
                    ans = "T√≥m t·∫Øt c√°c n·ªôi dung ch√≠nh:\n\n" + "\n\n".join(snippets)
                    sources = format_sources(found_many[:6])
                    print("\nü§ñ Chatbot:", ans, "\nüìö Ngu·ªìn:")
                    for s in sources:
                        if any(x in s.lower() for x in ["dataset", "n·ªôi b·ªô", "qa"]):
                            continue
                        print(" -", s)
                    print()
                    log_interaction(q, ans, sources)
                    continue
                else:
                    print("\nü§ñ Chatbot: Kh√¥ng t√¨m th·∫•y n·ªôi dung ƒë·ªÉ t√≥m t·∫Øt trong d·ªØ li·ªáu ph√°p l√Ω.\n")
                    log_interaction(q, "Kh√¥ng t√¨m th·∫•y n·ªôi dung ƒë·ªÉ t√≥m t·∫Øt", [])
                    continue

            if intent == "definition":
                # retrieve with dynamic top_k + BM25 priority for short defs
                try:
                    contexts, sources, raw = retrieve_multi(q, bm25, faiss_index, embed_model, meta)
                except Exception:
                    contexts, sources, raw = [], [], []
                if contexts:
                    prompt = build_prompt(q, contexts)
                    ans = generate_answer(prompt, tokenizer, model)
                    # If answer looks empty or garbage, fallback to scanning text files for definitions
                    if not ans or re.match(r'^[^a-zA-Z0-9\u00C0-\u024F]', ans) and len(ans) < 6:
                        # fallback scan
                        kw = re.escape(q.split()[0]) if q.split() else None
                        fallback_texts = []
                        if kw:
                            for root, _, files in os.walk(PROCESSED_DIR):
                                for f in files:
                                    if not f.lower().endswith((".txt", ".csv", ".json", ".parquet")):
                                        continue
                                    path = os.path.join(root, f)
                                    try:
                                        with open(path, "r", encoding="utf-8", errors="ignore") as fh:
                                            txt = fh.read(200000)
                                            if re.search(kw, txt, re.IGNORECASE):
                                                fallback_texts.append(clean_text(txt[:1000]))
                                                if len(fallback_texts) >= 3:
                                                    break
                                    except Exception:
                                        try:
                                            if f.lower().endswith(".parquet"):
                                                df = pd.read_parquet(path)
                                                txt2 = " ".join(df.astype(str).agg(" ".join, axis=1)[:200].tolist())
                                            else:
                                                df = pd.read_csv(path)
                                                txt2 = " ".join(df.astype(str).agg(" ".join, axis=1)[:200].tolist())
                                            if re.search(kw, txt2, re.IGNORECASE):
                                                fallback_texts.append(clean_text(txt2[:1000]))
                                                if len(fallback_texts) >= 3:
                                                    break
                                        except Exception:
                                            continue
                                if fallback_texts:
                                    break
                        if fallback_texts:
                            prompt = build_prompt(q, fallback_texts)
                            ans2 = generate_answer(prompt, tokenizer, model)
                            ans2 = ans2 or ans
                            print(f"\nü§ñ Chatbot: {ans2}\nüìö Ngu·ªìn:")
                            # choose friendly sources
                            print(" - (D·ªØ li·ªáu tham kh·∫£o n·ªôi b·ªô)")
                            print()
                            log_interaction(q, ans2, ["D·ªØ li·ªáu n·ªôi b·ªô"])
                            continue

                    # normal path
                    print(f"\nü§ñ Chatbot: {ans}\nüìö Ngu·ªìn:")
                    for s in sources:
                        if any(x in s.lower() for x in ["dataset", "n·ªôi b·ªô", "qa"]):
                            continue
                        print(" -", s)
                    print()
                    log_interaction(q, ans, sources)
                    continue
                else:
                    # fallback scan directly
                    kw = re.escape(q.split()[0]) if q.split() else None
                    fallback_texts = []
                    if kw:
                        for root, _, files in os.walk(PROCESSED_DIR):
                            for f in files:
                                if not f.lower().endswith((".txt", ".csv", ".json", ".parquet")):
                                    continue
                                path = os.path.join(root, f)
                                try:
                                    with open(path, "r", encoding="utf-8", errors="ignore") as fh:
                                        txt = fh.read(200000)
                                        if re.search(kw, txt, re.IGNORECASE):
                                            fallback_texts.append(clean_text(txt[:1000]))
                                            if len(fallback_texts) >= 3:
                                                break
                                except Exception:
                                    try:
                                        if f.lower().endswith(".parquet"):
                                            df = pd.read_parquet(path)
                                            txt2 = " ".join(df.astype(str).agg(" ".join, axis=1)[:200].tolist())
                                        else:
                                            df = pd.read_csv(path)
                                            txt2 = " ".join(df.astype(str).agg(" ".join, axis=1)[:200].tolist())
                                        if re.search(kw, txt2, re.IGNORECASE):
                                            fallback_texts.append(clean_text(txt2[:1000]))
                                            if len(fallback_texts) >= 3:
                                                break
                                    except Exception:
                                        continue
                            if fallback_texts:
                                break
                    if fallback_texts:
                        prompt = build_prompt(q, fallback_texts)
                        ans2 = generate_answer(prompt, tokenizer, model)
                        print(f"\nü§ñ Chatbot: {ans2}\nüìö Ngu·ªìn:")
                        print(" - (D·ªØ li·ªáu tham kh·∫£o n·ªôi b·ªô)")
                        print()
                        log_interaction(q, ans2, ["D·ªØ li·ªáu n·ªôi b·ªô"])
                        continue
                    else:
                        print("\nü§ñ Chatbot: Xin l·ªói, ch∆∞a t√¨m th·∫•y ƒë·ªãnh nghƒ©a/t√†i li·ªáu ph√π h·ª£p trong kho d·ªØ li·ªáu.\n")
                        log_interaction(q, "Kh√¥ng t√¨m th·∫•y ƒë·ªãnh nghƒ©a", [])
                        continue

        # FALLBACK: general RAG + LLM generation
        try:
            contexts, sources, raw = retrieve_multi(q, bm25, faiss_index, embed_model, meta)
        except Exception as e:
            contexts, sources, raw = [], [], []
        if not contexts:
            print("\nü§ñ Xin l·ªói, ch∆∞a t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p.\n")
            log_interaction(q, "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p", [])
            continue
        prompt = build_prompt(q, contexts)
        ans = generate_answer(prompt, tokenizer, model)
        print(f"\nü§ñ Chatbot: {ans}\nüìö Ngu·ªìn:")
        if sources:
            for s in sources:
                if any(x in s.lower() for x in ["dataset", "n·ªôi b·ªô", "qa"]):
                    continue
                print(" -", s)
        else:
            print(" - (N·ªôi dung ƒë∆∞·ª£c t·∫°o t·ª´ d·ªØ li·ªáu n·ªôi b·ªô; kh√¥ng c√≥ ngu·ªìn ph√°p l√Ω c·ª• th·ªÉ.)")
        print()
        log_interaction(q, ans, sources)


# Entry
if __name__ == "__main__":
    try:
        chat()
    except Exception as e:
        print("L·ªói ch∆∞∆°ng tr√¨nh:", e)
        traceback.print_exc()  

     

